{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# CELL 1: FIX DEPENDENCIES - RUN FIRST AFTER RESTART\n# ============================================================================\n\nimport subprocess\nimport sys\n\nprint('🔧 Fixing package dependencies...')\n\n# Fix PyArrow compatibility - install version 15.0.2 for bigframes\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"pyarrow\"], \n                capture_output=True, check=False)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pyarrow==15.0.2\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Fix rich version for bigframes compatibility\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"rich==13.7.1\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Install google-cloud-bigquery-storage (missing dependency)\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"google-cloud-bigquery-storage>=2.30.0\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Upgrade google-cloud-bigquery for bigframes\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"google-cloud-bigquery>=3.31.0\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Upgrade google-api-core for pandas-gbq\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"google-api-core>=2.10.2\", \"--no-cache-dir\", \"-q\"], \n                check=True)\n\n# Install missing packages including faiss-cpu and fix protobuf\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"keybert\", \"rank-bm25\", \"evaluate\", \"faiss-cpu\", \"protobuf<5.0.0\"], \n                check=True)\n\nprint('✅ Dependencies fixed and installed')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-25T06:00:11.411827Z","iopub.execute_input":"2025-10-25T06:00:11.412115Z","iopub.status.idle":"2025-10-25T06:02:10.406134Z","shell.execute_reply.started":"2025-10-25T06:00:11.412094Z","shell.execute_reply":"2025-10-25T06:02:10.405342Z"}},"outputs":[{"name":"stdout","text":"🔧 Fixing package dependencies...\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.3/38.3 MB 311.6 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 15.0.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.7/240.7 kB 9.7 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.6/293.6 kB 9.1 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 259.3/259.3 kB 9.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.1/167.1 kB 118.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 138.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 323.2/323.2 kB 321.1 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.27.0 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.4/41.4 kB 2.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 5.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.4/31.4 MB 56.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.9/294.9 kB 19.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.7/47.7 MB 38.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 96.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 68.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 46.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 1.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 2.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 31.8 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 13.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 8.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 76.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 566.1/566.1 kB 26.1 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngrpcio-status 1.76.0 requires protobuf<7.0.0,>=6.31.1, but you have protobuf 4.25.8 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.27.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"✅ Dependencies fixed and installed\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: VERIFY ALL IMPORTS - RUN SECOND\n# ============================================================================\n!pip install sacremoses\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"🔍 Testing imports...\")\n\ntry:\n    from datasets import load_dataset\n    print(\"✅ datasets\")\n    from sentence_transformers import SentenceTransformer\n    print(\"✅ sentence-transformers\")\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    print(\"✅ transformers\")\n    import faiss\n    print(\"✅ faiss\")\n    from keybert import KeyBERT\n    print(\"✅ keybert\")\n    from rank_bm25 import BM25Okapi\n    print(\"✅ rank-bm25\")\n    import torch\n    print(f\"✅ torch (device: {'cuda' if torch.cuda.is_available() else 'cpu'})\")\n    print(\"\\n🎉 ALL IMPORTS SUCCESSFUL!\")\nexcept Exception as e:\n    print(f\"❌ Import failed: {e}\\nPlease restart kernel and try again.\")\n    raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T06:02:36.974466Z","iopub.execute_input":"2025-10-25T06:02:36.975139Z","iopub.status.idle":"2025-10-25T06:03:08.405568Z","shell.execute_reply.started":"2025-10-25T06:02:36.975099Z","shell.execute_reply":"2025-10-25T06:03:08.404895Z"}},"outputs":[{"name":"stdout","text":"Collecting sacremoses\n  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacremoses) (2025.9.18)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from sacremoses) (8.3.0)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from sacremoses) (1.5.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sacremoses) (4.67.1)\nDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sacremoses\nSuccessfully installed sacremoses-0.1.1\n🔍 Testing imports...\n✅ datasets\n","output_type":"stream"},{"name":"stderr","text":"2025-10-25 06:02:54.338237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761372174.563259      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761372174.630125      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"✅ sentence-transformers\n✅ transformers\n✅ faiss\n✅ keybert\n✅ rank-bm25\n✅ torch (device: cuda)\n\n🎉 ALL IMPORTS SUCCESSFUL!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# MULTI-DOMAIN RAG PIPELINE FOR MEDICAL QA\n# Complete production-ready implementation\n# Datasets: Women's Health + General Medical QA\n# Models: all-MiniLM (embedder), BGE-reranker, BioGPT (HyDE), Flan-T5 (generator)\n# ============================================================================\n\nimport os, sys, time, json, pickle, re, warnings, random\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np\nimport torch\nfrom pathlib import Path\nfrom dataclasses import dataclass, asdict\nfrom typing import Dict, List, Optional, Any, Tuple\nimport logging\nfrom datetime import datetime\n\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\nfrom keybert import KeyBERT\nimport faiss\nfrom rank_bm25 import BM25Okapi\n\n# ============================================================================\n# REPRODUCIBILITY & SETUP\n# ============================================================================\n\ndef set_all_seeds(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_all_seeds(42)\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Download NLTK data\nfor resource in ['punkt', 'stopwords']:\n    try:\n        nltk.data.find(f'tokenizers/{resource}')\n    except LookupError:\n        nltk.download(resource, quiet=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogger.info(f\"🚀 Device: {device}\")\nif torch.cuda.is_available():\n    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}, Memory: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n@dataclass\nclass DomainConfig:\n    name: str\n    dataset_name: str\n    config_name: str = None  # ← ADD THIS LINE\n    dataset_split: str = \"train\"\n    index_path: str = None\n    id2doc_path: str = None\n    metadata_path: str = None\n\n    \n    def __post_init__(self):\n        if self.index_path is None:\n            self.index_path = f\"{self.name}_faiss.index\"\n        if self.id2doc_path is None:\n            self.id2doc_path = f\"{self.name}_id2doc.pkl\"\n        if self.metadata_path is None:\n            self.metadata_path = f\"{self.name}_metadata.json\"\n\n@dataclass\nclass RAGConfig:\n    embed_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    reranker_model: str = \"BAAI/bge-reranker-large\"\n    hyde_model: str = \"gpt2\"\n    generator_model: str = \"microsoft/BioGPT-Large\"\n    \n    chunk_window: int = 3\n    chunk_stride: int = 1\n    retrieve_k: int = 30\n    rerank_topk: int = 8\n    context_chunks: int = 4\n    hyde_weight: float = 0.4\n    faiss_alpha: float = 0.6\n    \n    max_new_tokens: int = 200\n    hyde_max_tokens: int = 60\n    \n    completeness_threshold: float = 0.65\n    faithfulness_threshold: float = 0.55\n    \n    retrieval_weight: float = 0.4\n    completeness_weight: float = 0.3\n    faithfulness_weight: float = 0.3\n    \n    prompts_log: str = \"prompts_outputs.pkl\"\n    random_seed: int = 42\n    test_size: float = 0.15\n\nDOMAINS = [\n    DomainConfig(name=\"women_health\", dataset_name=\"altaidevorg/women-health-mini\"),\n    DomainConfig(name=\"medical_qa\", dataset_name=\"Malikeh1375/medical-question-answering-datasets\", config_name=\"all-processed\")\n\n]\n\nconfig = RAGConfig()\n\n# ============================================================================\n# UTILITIES\n# ============================================================================\n\ndef clean_text_artifacts(text: str) -> str:\n    text = re.sub(r\"^(Answer:|Final answer:|Response:)\\s*\", \"\", text, flags=re.IGNORECASE)\n    text = re.sub(r\"<\\/?[^>]+>|</s>|▃|\\[INST\\]|\\[/INST\\]\", \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text.strip(\" \\n\\r\\t\\\"'\")\n\ndef monitor_memory():\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        logger.info(f\"💾 GPU: {allocated:.2f}GB / {total:.2f}GB ({allocated/total*100:.1f}%)\")\n        if allocated/total > 0.85:\n            torch.cuda.empty_cache()\n\n# ============================================================================\n# DATA LOADING\n# ============================================================================\n\nclass DatasetLoader:\n    @staticmethod\n    def extract_qa_pairs(dataset, domain_name: str) -> List[Dict[str, Any]]:\n        qa_data = []\n        for idx, row in enumerate(dataset):\n            try:\n                conv = None\n                if isinstance(row, dict):\n                    for field in [\"conversations\", \"conversation\", \"dialog\", \"dialogue\", \"messages\", \"turns\"]:\n                        if field in row:\n                            conv = row[field]\n                            break\n                \n                if not conv:\n                    continue\n                \n                user_msgs, assistant_msgs = [], []\n                \n                if isinstance(conv, list) and len(conv) > 0:\n                    if isinstance(conv[0], dict):\n                        if \"from\" in conv[0] and \"value\" in conv[0]:\n                            user_msgs = [m[\"value\"] for m in conv if m.get(\"from\") in (\"human\", \"user\")]\n                            assistant_msgs = [m[\"value\"] for m in conv if m.get(\"from\") in (\"assistant\", \"bot\", \"system\")]\n                        elif \"role\" in conv[0] and \"content\" in conv[0]:\n                            user_msgs = [m[\"content\"] for m in conv if m.get(\"role\") in (\"user\", \"human\")]\n                            assistant_msgs = [m[\"content\"] for m in conv if m.get(\"role\") in (\"assistant\", \"bot\")]\n                    else:\n                        if len(conv) >= 2:\n                            user_msgs, assistant_msgs = [conv[0]], conv[1:]\n                \n                if user_msgs and assistant_msgs:\n                    question = \" \".join(user_msgs).strip()\n                    answer = \" \".join(assistant_msgs).strip()\n                    if question and answer and len(question) > 10 and len(answer) > 10:\n                        qa_data.append({\n                            \"question\": question,\n                            \"answer\": answer,\n                            \"domain\": domain_name,\n                            \"source_id\": idx\n                        })\n            except Exception:\n                continue\n        return qa_data\n    \n    @staticmethod\n    def load_domain_data(domain_config: DomainConfig) -> Tuple[List[Dict], List[Dict]]:\n        logger.info(f\"📥 Loading {domain_config.name}...\")\n        try:\n            dataset = load_dataset(domain_config.dataset_name, domain_config.config_name, split=domain_config.dataset_split)\n\n            qa_data = DatasetLoader.extract_qa_pairs(dataset, domain_config.name)\n            \n            if not qa_data:\n                raise ValueError(f\"No QA pairs extracted\")\n            \n            train_data, test_data = train_test_split(\n                qa_data, test_size=config.test_size, random_state=config.random_seed\n            )\n            logger.info(f\"✅ {domain_config.name}: {len(train_data)} train, {len(test_data)} test\")\n            return train_data, test_data\n        except Exception as e:\n            logger.error(f\"❌ Failed to load {domain_config.name}: {e}\")\n            raise\n\n# ============================================================================\n# TEXT CHUNKING\n# ============================================================================\n\nclass TextChunker:\n    @staticmethod\n    def create_chunks(data: List[Dict], window: int = 3, stride: int = 1, min_chars: int = 50) -> List[Dict]:\n        chunks = []\n        for item in data:\n            text = item.get(\"answer\", \"\")\n            if not text or len(text) < min_chars:\n                continue\n            \n            sentences = sent_tokenize(text)\n            if not sentences:\n                continue\n            \n            if len(sentences) <= window:\n                chunks.append({\n                    \"chunk\": \" \".join(sentences),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks)\n                })\n                continue\n            \n            for i in range(0, max(1, len(sentences) - window + 1), stride):\n                chunks.append({\n                    \"chunk\": \" \".join(sentences[i:i + window]),\n                    \"source_idx\": item.get(\"source_id\", -1),\n                    \"domain\": item.get(\"domain\", \"unknown\"),\n                    \"chunk_id\": len(chunks),\n                    \"window\": (i, i + window)\n                })\n        return chunks\n\n# ============================================================================\n# MODEL MANAGEMENT\n# ============================================================================\n\nclass ModelManager:\n    def __init__(self, config: RAGConfig, device: torch.device):\n        self.config = config\n        self.device = device\n        self.models = {}\n    \n    def load_embedder(self):\n        logger.info(f\"📦 Loading embedder...\")\n        embedder = SentenceTransformer(self.config.embed_model, device=self.device)\n        self.models['embedder'] = embedder\n        logger.info(f\"✅ Embedder loaded\")\n        return embedder\n    \n    def load_reranker(self):\n        logger.info(f\"📦 Loading reranker...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.reranker_model)\n        model = AutoModelForSequenceClassification.from_pretrained(\n            self.config.reranker_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n        ).to(self.device)\n        model.eval()\n        self.models['reranker_tokenizer'] = tokenizer\n        self.models['reranker_model'] = model\n        logger.info(f\"✅ Reranker loaded\")\n        return tokenizer, model\n    \n    def load_hyde_model(self):\n        logger.info(f\"📦 Loading HyDE model...\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(self.config.hyde_model)\n            model = AutoModelForCausalLM.from_pretrained(\n                self.config.hyde_model,\n                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n                low_cpu_mem_usage=True\n            ).to(self.device)\n            model.eval()\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            self.models['hyde_tokenizer'] = tokenizer\n            self.models['hyde_model'] = model\n            logger.info(f\"✅ HyDE model loaded\")\n            return tokenizer, model\n        except Exception as e:\n            logger.warning(f\"⚠️ HyDE load failed, using query expansion: {e}\")\n            return None, None\n    \n    def load_generator(self):\n        logger.info(f\"📦 Loading generator...\")\n        tokenizer = AutoTokenizer.from_pretrained(self.config.generator_model)\n        model = AutoModelForCausalLM.from_pretrained(\n            self.config.generator_model,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            low_cpu_mem_usage=True\n        ).to(self.device)\n        model.eval()\n        self.models['gen_tokenizer'] = tokenizer\n        self.models['gen_model'] = model\n        logger.info(f\"✅ Generator loaded\")\n        return tokenizer, model\n    \n    def load_keyword_extractor(self):\n        try:\n            kw_model = KeyBERT(model=self.models.get('embedder'))\n            self.models['keyword_extractor'] = kw_model\n            logger.info(f\"✅ KeyBERT loaded\")\n            return kw_model\n        except Exception as e:\n            logger.warning(f\"⚠️ KeyBERT load failed: {e}\")\n            return None\n    \n    def load_all(self):\n        logger.info(\"🔧 Loading all models...\")\n        self.load_embedder()\n        self.load_reranker()\n        self.load_hyde_model()\n        self.load_generator()\n        self.load_keyword_extractor()\n        monitor_memory()\n        logger.info(\"✅ All models loaded\")\n        return self.models\n\n# ============================================================================\n# INDEX MANAGEMENT\n# ============================================================================\n\nclass MultiDomainIndexManager:\n    def __init__(self, config: RAGConfig, embedder: SentenceTransformer):\n        self.config = config\n        self.embedder = embedder\n        self.domain_indices = {}\n    \n    def build_or_load_domain_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        if Path(domain_config.index_path).exists() and Path(domain_config.id2doc_path).exists():\n            try:\n                return self._load_existing_index(domain_config)\n            except:\n                pass\n        return self._build_new_index(domain_config, chunks)\n    \n    def _load_existing_index(self, domain_config: DomainConfig) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"📂 Loading existing {domain_config.name} index...\")\n        index = faiss.read_index(domain_config.index_path)\n        with open(domain_config.id2doc_path, \"rb\") as f:\n            id2doc = pickle.load(f)\n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        logger.info(f\"✅ Loaded {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def _build_new_index(self, domain_config: DomainConfig, chunks: List[Dict]) -> Tuple[faiss.Index, List[str], BM25Okapi]:\n        logger.info(f\"🔨 Building {domain_config.name} index...\")\n        id2doc = [chunk[\"chunk\"] for chunk in chunks]\n        \n        embeddings = self.embedder.encode(\n            id2doc, normalize_embeddings=True, show_progress_bar=True,\n            batch_size=64, convert_to_numpy=True\n        ).astype('float32')\n        \n        dim = embeddings.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(embeddings)\n        \n        bm25_corpus = [word_tokenize(doc.lower()) for doc in id2doc]\n        bm25 = BM25Okapi(bm25_corpus)\n        \n        faiss.write_index(index, domain_config.index_path)\n        with open(domain_config.id2doc_path, \"wb\") as f:\n            pickle.dump(id2doc, f)\n        \n        metadata = {\"created_at\": time.time(), \"n_vectors\": int(index.ntotal), \"embedding_dim\": dim, \"domain\": domain_config.name}\n        with open(domain_config.metadata_path, \"w\") as f:\n            json.dump(metadata, f, indent=2)\n        \n        logger.info(f\"✅ Built {domain_config.name}: {index.ntotal} vectors\")\n        return index, id2doc, bm25\n    \n    def load_all_domains(self, domain_chunks: Dict[str, List[Dict]]):\n        for domain in DOMAINS:\n            index, id2doc, bm25 = self.build_or_load_domain_index(domain, domain_chunks.get(domain.name, []))\n            self.domain_indices[domain.name] = {\n                'index': index, 'id2doc': id2doc, 'bm25': bm25, 'config': domain\n            }\n        logger.info(f\"✅ Loaded {len(self.domain_indices)} domain indices\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T06:03:15.312764Z","iopub.execute_input":"2025-10-25T06:03:15.313448Z","iopub.status.idle":"2025-10-25T06:03:15.894433Z","shell.execute_reply.started":"2025-10-25T06:03:15.313424Z","shell.execute_reply":"2025-10-25T06:03:15.893880Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# QUERY ROUTER\n# ============================================================================\n\nclass QueryRouter:\n    def __init__(self, embedder: SentenceTransformer, domain_indices: Dict):\n        self.embedder = embedder\n        self.domain_indices = domain_indices\n        self.domain_centroids = self._compute_centroids()\n    \n    def _compute_centroids(self) -> Dict[str, np.ndarray]:\n        centroids = {}\n        logger.info(\"🎯 Computing domain centroids...\")\n        for domain_name, domain_data in self.domain_indices.items():\n            id2doc = domain_data['id2doc']\n            sample_docs = random.sample(id2doc, min(500, len(id2doc)))\n            embeddings = self.embedder.encode(sample_docs, normalize_embeddings=True, convert_to_numpy=True)\n            centroids[domain_name] = embeddings.mean(axis=0)\n        return centroids\n    \n    def route_query(self, query: str, top_k: int = 2) -> List[str]:\n        query_emb = self.embedder.encode([query], normalize_embeddings=True, convert_to_numpy=True)[0]\n        similarities = {domain: float(np.dot(query_emb, centroid)) for domain, centroid in self.domain_centroids.items()}\n        sorted_domains = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n        selected = [d[0] for d in sorted_domains[:top_k]]\n        logger.info(f\"🧭 Routed to: {selected}\")\n        return selected\n\n# ============================================================================\n# RAG PIPELINE\n# ============================================================================\n\nclass MultiDomainRAGPipeline:\n    def __init__(self, config: RAGConfig, domains: List[DomainConfig]):\n        self.config = config\n        self.domains = domains\n        self.device = device\n        \n        self.model_manager = ModelManager(config, device)\n        self.models = self.model_manager.load_all()\n        \n        self.data = {}\n        self.test_data = {}\n        domain_chunks = {}\n        \n        for domain in domains:\n            train_data, test_data = DatasetLoader.load_domain_data(domain)\n            self.data[domain.name] = train_data\n            self.test_data[domain.name] = test_data\n            chunks = TextChunker.create_chunks(train_data, window=config.chunk_window, stride=config.chunk_stride)\n            domain_chunks[domain.name] = chunks\n        \n        self.index_manager = MultiDomainIndexManager(config, self.models['embedder'])\n        self.index_manager.load_all_domains(domain_chunks)\n        \n        self.router = QueryRouter(self.models['embedder'], self.index_manager.domain_indices)\n        self.prompts_log = []\n        \n        logger.info(\"✅ Multi-domain RAG pipeline initialized\")\n    \n    def generate_hyde_answer(self, query: str) -> str:\n        if self.models['hyde_model'] is None:\n            return query\n        \n        prompt = f\"Question: {query}\\nAnswer:\"\n        try:\n            inputs = self.models['hyde_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['hyde_model'].generate(\n                    **inputs, max_new_tokens=self.config.hyde_max_tokens,\n                    do_sample=False, pad_token_id=self.models['hyde_tokenizer'].eos_token_id,\n                    repetition_penalty=1.15\n                )\n            text = self.models['hyde_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            hyde = clean_text_artifacts(text.split(\"Answer:\")[-1])\n            return hyde if hyde else query\n        except:\n            return query\n    \n    def retrieve_from_domain(self, query: str, domain_name: str, k: int) -> List[Tuple[int, float, str]]:\n        domain_data = self.index_manager.domain_indices[domain_name]\n        index = domain_data['index']\n        id2doc = domain_data['id2doc']\n        bm25 = domain_data['bm25']\n        \n        hyde_text = self.generate_hyde_answer(query)\n        q_emb = self.models['embedder'].encode([query], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        h_emb = self.models['embedder'].encode([hyde_text], normalize_embeddings=True, convert_to_numpy=True).astype('float32')\n        merged_emb = (1 - self.config.hyde_weight) * q_emb + self.config.hyde_weight * h_emb\n        \n        D, I = index.search(merged_emb, k)\n        faiss_scores = D[0]\n        if faiss_scores.max() > faiss_scores.min():\n            faiss_norm = (faiss_scores - faiss_scores.min()) / (faiss_scores.max() - faiss_scores.min())\n        else:\n            faiss_norm = np.ones_like(faiss_scores)\n        faiss_map = {int(idx): float(score) for idx, score in zip(I[0], faiss_norm)}\n        \n        bm25_scores = bm25.get_scores(word_tokenize(query.lower()))\n        if bm25_scores.max() > bm25_scores.min():\n            bm25_norm = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min())\n        else:\n            bm25_norm = np.zeros_like(bm25_scores)\n        \n        candidates = set(I[0].tolist()) | set(np.argsort(bm25_scores)[::-1][:k].tolist())\n        merged_scores = []\n        for idx in candidates:\n            f = faiss_map.get(int(idx), 0.0)\n            b = float(bm25_norm[int(idx)]) if int(idx) < len(bm25_norm) else 0.0\n            score = self.config.faiss_alpha * f + (1 - self.config.faiss_alpha) * b\n            merged_scores.append((int(idx), score, domain_name))\n        \n        merged_scores.sort(key=lambda x: x[1], reverse=True)\n        return merged_scores[:k]\n    \n    def rerank_candidates(self, query: str, candidates: List[Tuple[int, float, str]]) -> List[Tuple[str, float, str]]:\n        texts, metadata = [], []\n        for idx, score, domain_name in candidates:\n            domain_data = self.index_manager.domain_indices[domain_name]\n            text = domain_data['id2doc'][idx]\n            texts.append(text)\n            metadata.append((idx, domain_name))\n        \n        reranker_scores = []\n        batch_size = 8\n        for i in range(0, len(texts), batch_size):\n            batch_texts = texts[i:i+batch_size]\n            inputs = self.models['reranker_tokenizer'](\n                [query] * len(batch_texts), batch_texts,\n                padding=True, truncation=True, max_length=512, return_tensors=\"pt\"\n            ).to(self.device)\n            \n            with torch.no_grad():\n                outputs = self.models['reranker_model'](**inputs)\n                logits = outputs.logits.cpu().numpy()\n            \n            for lg in logits:\n                if lg.shape == ():\n                    score = float(lg)\n                elif len(lg.shape) == 1 and lg.shape[0] == 1:\n                    score = float(lg[0])\n                elif len(lg.shape) == 1 and lg.shape[0] == 2:\n                    score = float(lg[1])\n                else:\n                    score = float(np.max(lg))\n                reranker_scores.append(score)\n        \n        reranked = [(texts[i], reranker_scores[i], metadata[i][1]) for i in range(len(texts))]\n        reranked.sort(key=lambda x: x[1], reverse=True)\n        return reranked[:self.config.rerank_topk]\n    \n    def generate_answer(self, query: str, contexts: List[Tuple[str, float, str]]) -> str:\n        context_parts = [f\"[Source {i+1} from {domain}]:\\n{text}\" \n                        for i, (text, score, domain) in enumerate(contexts[:self.config.context_chunks])]\n        context_block = \"\\n\\n\".join(context_parts)\n        \n        prompt = f\"\"\"Based on the following medical information, answer the question concisely and accurately.\n\n{context_block}\n\nQuestion: {query}\n\nAnswer:\"\"\"\n        \n        try:\n            inputs = self.models['gen_tokenizer'](prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(self.device)\n            with torch.no_grad():\n                outputs = self.models['gen_model'].generate(\n                    **inputs, max_new_tokens=self.config.max_new_tokens,\n                    do_sample=False, pad_token_id=self.models['gen_tokenizer'].eos_token_id,\n                    repetition_penalty=1.1\n                )\n            raw = self.models['gen_tokenizer'].decode(outputs[0], skip_special_tokens=True)\n            answer = clean_text_artifacts(raw.split(\"Answer:\")[-1])\n            \n            self.prompts_log.append({\n                \"type\": \"generate\", \"query\": query,\n                \"contexts\": [(t, d) for t, _, d in contexts[:self.config.context_chunks]],\n                \"prompt\": prompt, \"raw\": raw, \"answer\": answer, \"timestamp\": time.time()\n            })\n            \n            return answer if answer else \"Insufficient information.\"\n        except Exception as e:\n            logger.error(f\"Generation failed: {e}\")\n            return \"Error generating answer.\"\n    \n    def compute_metrics(self, query: str, answer: str, contexts: List[Tuple[str, float, str]]) -> Dict[str, float]:\n        metrics = {}\n        \n        if contexts:\n            retrieval_score = np.mean([score for _, score, _ in contexts[:self.config.context_chunks]])\n            metrics['retrieval'] = float(retrieval_score)\n        else:\n            metrics['retrieval'] = 0.0\n        \n        try:\n            context_texts = [text for text, _, _ in contexts[:self.config.context_chunks]]\n            all_keywords = []\n            \n            if self.models['keyword_extractor']:\n                for ctx_text in context_texts:\n                    keywords = self.models['keyword_extractor'].extract_keywords(\n                        ctx_text, keyphrase_ngram_range=(1, 2), stop_words='english', top_n=5\n                    )\n                    all_keywords.extend([kw for kw, _ in keywords])\n            \n            unique_keywords = list(dict.fromkeys([kw.lower() for kw in all_keywords if kw]))\n            \n            if unique_keywords and answer:\n                answer_emb = self.models['embedder'].encode([answer], normalize_embeddings=True, convert_to_tensor=True)\n                keyword_embs = self.models['embedder'].encode(unique_keywords, normalize_embeddings=True, convert_to_tensor=True)\n                similarities = util.cos_sim(answer_emb, keyword_embs).cpu().numpy()[0]\n                covered = (similarities >= self.config.completeness_threshold).sum()\n                metrics['completeness'] = float(covered / len(unique_keywords))\n            else:\n                metrics['completeness'] = 0.0\n        except:\n            metrics['completeness'] = 0.0\n        \n        try:\n            if answer and contexts:\n                answer_sentences = sent_tokenize(answer)\n                context_sentences = []\n                for text, _, _ in contexts[:self.config.context_chunks]:\n                    context_sentences.extend(sent_tokenize(text))\n                \n                if answer_sentences and context_sentences:\n                    ans_embs = self.models['embedder'].encode(answer_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    ctx_embs = self.models['embedder'].encode(context_sentences, normalize_embeddings=True, convert_to_tensor=True)\n                    sim_matrix = util.cos_sim(ans_embs, ctx_embs).cpu().numpy()\n                    max_sims = np.max(sim_matrix, axis=1)\n                    faithful = (max_sims >= self.config.faithfulness_threshold).sum()\n                    metrics['faithfulness'] = float(faithful / len(answer_sentences))\n                else:\n                    metrics['faithfulness'] = 0.0\n            else:\n                metrics['faithfulness'] = 0.0\n        except:\n            metrics['faithfulness'] = 0.0\n        \n        metrics['composite'] = (\n            self.config.retrieval_weight * metrics['retrieval'] +\n            self.config.completeness_weight * metrics['completeness'] +\n            self.config.faithfulness_weight * metrics['faithfulness']\n        )\n        \n        return metrics\n    \n    def run_query(self, query: str, top_domains: int = 2, log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"🔍 Processing: {query[:100]}...\")\n        \n        selected_domains = self.router.route_query(query, top_k=top_domains)\n        \n        all_candidates = []\n        for domain_name in selected_domains:\n            candidates = self.retrieve_from_domain(query, domain_name, k=self.config.retrieve_k)\n            all_candidates.extend(candidates)\n        \n        if log_diagnostics:\n            logger.info(f\"Retrieved {len(all_candidates)} candidates from {len(selected_domains)} domains\")\n        \n        reranked = self.rerank_candidates(query, all_candidates)\n        \n        if log_diagnostics:\n            logger.info(\"Top reranked contexts:\")\n            for i, (text, score, domain) in enumerate(reranked[:3]):\n                logger.info(f\"  {i+1}. [{domain}] (score={score:.3f}): {text[:150]}...\")\n        \n        answer = self.generate_answer(query, reranked)\n        metrics = self.compute_metrics(query, answer, reranked)\n        \n        result = {\n            \"query\": query,\n            \"routed_domains\": selected_domains,\n            \"answer\": answer,\n            \"contexts\": [(text, domain) for text, _, domain in reranked[:self.config.context_chunks]],\n            \"metrics\": metrics\n        }\n        \n        return result\n    \n    def evaluate_batch(self, queries: List[str], log_diagnostics: bool = False) -> Dict[str, Any]:\n        logger.info(f\"📊 Evaluating {len(queries)} queries...\")\n        \n        results = []\n        failed = []\n        \n        for i, query in enumerate(queries):\n            try:\n                result = self.run_query(query, log_diagnostics=log_diagnostics)\n                results.append(result)\n                \n                if (i + 1) % 3 == 0:\n                    logger.info(f\"Progress: {i+1}/{len(queries)}\")\n                    monitor_memory()\n            except Exception as e:\n                logger.error(f\"Failed query {i}: {e}\")\n                failed.append((i, query, str(e)))\n        \n        if not results:\n            return {\"error\": \"No successful queries\"}\n        \n        avg_metrics = {\n            \"retrieval\": np.mean([r[\"metrics\"][\"retrieval\"] for r in results]),\n            \"completeness\": np.mean([r[\"metrics\"][\"completeness\"] for r in results]),\n            \"faithfulness\": np.mean([r[\"metrics\"][\"faithfulness\"] for r in results]),\n            \"composite\": np.mean([r[\"metrics\"][\"composite\"] for r in results])\n        }\n        \n        summary = {\n            \"total_queries\": len(queries),\n            \"successful\": len(results),\n            \"failed\": len(failed),\n            \"success_rate\": len(results) / len(queries),\n            \"average_metrics\": avg_metrics,\n            \"failed_queries\": failed,\n            \"individual_results\": results\n        }\n        \n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        results_file = f\"evaluation_{timestamp}.json\"\n        try:\n            with open(results_file, \"w\") as f:\n                json.dump(summary, f, indent=2, default=str)\n            logger.info(f\"💾 Results saved to {results_file}\")\n        except:\n            pass\n        \n        return summary\n\n# ============================================================================\n# PIPELINE EXECUTION\n# ============================================================================\n\nlogger.info(\"=\"*80)\nlogger.info(\"🚀 INITIALIZING MULTI-DOMAIN RAG PIPELINE\")\nlogger.info(\"=\"*80)\n\nrag_pipeline = MultiDomainRAGPipeline(config, DOMAINS)\n\ntest_queries = [\n    \"What are the recommended health screenings for women in their 40s?\",\n    \"Explain the symptoms and management of preeclampsia.\",\n    \"What are the early warning signs of Parkinson's disease?\",\n    \"How is PCOS diagnosed and treated?\",\n    \"What are the differences between Type 1 and Type 2 diabetes?\"\n]\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"🧪 RUNNING SINGLE QUERY TEST WITH DIAGNOSTICS\")\nlogger.info(\"=\"*80)\n\ntest_query = test_queries[0]\nresult = rag_pipeline.run_query(test_query, top_domains=2, log_diagnostics=True)\n\nlogger.info(f\"\\n{'='*80}\")\nlogger.info(f\"📋 QUERY: {result['query']}\")\nlogger.info(f\"{'='*80}\")\nlogger.info(f\"🎯 Routed to: {result['routed_domains']}\")\nlogger.info(f\"\\n✅ ANSWER:\\n{result['answer']}\")\nlogger.info(f\"\\n📊 METRICS:\")\nfor metric_name, value in result['metrics'].items():\n    logger.info(f\"   {metric_name}: {value:.3f}\")\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"📊 RUNNING BATCH EVALUATION\")\nlogger.info(\"=\"*80)\n\nbatch_results = rag_pipeline.evaluate_batch(test_queries[:3], log_diagnostics=False)\n\nlogger.info(f\"\\n{'='*80}\")\nlogger.info(\"📈 BATCH EVALUATION SUMMARY\")\nlogger.info(f\"{'='*80}\")\nlogger.info(f\"Success Rate: {batch_results['success_rate']:.1%}\")\nlogger.info(f\"Average Retrieval: {batch_results['average_metrics']['retrieval']:.3f}\")\nlogger.info(f\"Average Completeness: {batch_results['average_metrics']['completeness']:.3f}\")\nlogger.info(f\"Average Faithfulness: {batch_results['average_metrics']['faithfulness']:.3f}\")\nlogger.info(f\"Average Composite: {batch_results['average_metrics']['composite']:.3f}\")\n\nlogger.info(\"\\n\" + \"=\"*80)\nlogger.info(\"✅ MULTI-DOMAIN RAG PIPELINE COMPLETE\")\nlogger.info(\"=\"*80)\n\ntry:\n    with open(config.prompts_log, \"wb\") as f:\n        pickle.dump(rag_pipeline.prompts_log, f)\n    logger.info(f\"📝 Prompt logs saved to {config.prompts_log}\")\nexcept:\n    pass\n\nmonitor_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T06:03:59.494852Z","iopub.execute_input":"2025-10-25T06:03:59.495165Z","iopub.status.idle":"2025-10-25T06:06:13.046765Z","shell.execute_reply.started":"2025-10-25T06:03:59.495142Z","shell.execute_reply":"2025-10-25T06:06:13.045749Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1076e6f9bc074f3f93c76ca334989e1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4d4201a010646a78597a88676a22a69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f7efff21c146af95f3c80617db4caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7c8a87ea15430cb0986d23002d7d9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da3cea2dc5f461294b7b1032cf02022"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e04ca3277dc44b59657a70a3f595290"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d836de2b0ec5423b9e93a41fdac86dea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a28e908c7544bd6a91dfd90c552e2d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74610ceb5e546bbab1504ce10d6f50e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fb807d0c65840458c7546c55e0ae13a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f46032b9cd774585a43f7211ed0687cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5712b86521bc454ea3ea194a7da24396"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e26c33546146436798f7a580d89b35d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad09a7033c324cc8a7734a47866341db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f2bba451bab4b11b40139f3c23562a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27cd4ec9e6f24d7f9af39ecef390039e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67f4323097dd4337ad5b6fdb4513fbe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2cdd0577fda441d905551ecbc348cf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d11f87f71c3a48719b231533e03da39e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e4be22db98646009b1b400de876e207"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"227130d671054f16992564eb5708613b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"135e3af9563c40c59b20ae2614facd0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f23a43b6d7b4ed9982906961ae9aeea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1901965597704b5db98c64eb86f3136b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/256 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ab84308d634135bebecb145e32c23e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"436e521d72c64f38a0dc3768ba8ee205"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07eedc2ae43c41019d7dedf3fb0ee0cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7162f58d55b0453895999024c180a304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/658 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e45adef1d6114edb9c4f99fd7735b62e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b4ddee8accd4b3ab55f0e7f12327e26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a5d6ec8c854281ae291831a412807b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c5a18152024cd893c71457cf81a21f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763c35320621423cbdcb88df7ccc000a"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"women-health-mini.jsonl:   0%|          | 0.00/35.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d9af35429b444aa3c392f73bd94611"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10348 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f43520265def4d8ea5ba09ae3b97f15a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4728665411c4df49baa37a9ed0ddd05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"all-processed/train-00000-of-00001-9bfe4(…):   0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0f8b4ca40c54ef2961f52759f80dec0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/246678 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4450038390b64384907ab9881417a4ae"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/2786040155.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m \u001b[0mrag_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiDomainRAGPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOMAINS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m test_queries = [\n","\u001b[0;32m/tmp/ipykernel_37/2786040155.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, domains)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdomain\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdomains\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_domain_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_37/3412756147.py\u001b[0m in \u001b[0;36mload_domain_data\u001b[0;34m(domain_config)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mqa_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No QA pairs extracted\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             train_data, test_data = train_test_split(\n","\u001b[0;31mValueError\u001b[0m: No QA pairs extracted"],"ename":"ValueError","evalue":"No QA pairs extracted","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}